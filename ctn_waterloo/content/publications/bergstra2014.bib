@inproceedings{bergstra2014,
  title={Preliminary Evaluation of Hyperopt Algorithms on HPOLib},
  author={James Bergstra and Brent Komer and Chris Eliasmith and David Warde-Farley},
  year={2014},
  url={https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxhdXRvbWx3c2ljbWwxNHxneDo0MTIxNTdlNTFlM2Q5ZDkx},
  pdf={http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.pdf},
  poster={http://compneuro.uwaterloo.ca/files/publications/bergstra.2014.poster.pdf},
  booktitle={ICML 2014 AutoML Workshop},
  pages={7},
  abstract={Model selection, also known as hyperparameter tuning, can be viewed as a blackbox optimization problem. Recently the HPOlib benchmarking suite was advanced to facilitate algorithm comparison between hyperparameter optimization algorithms. We compare seven optimization algorithms implemented in the Hyperopt optimization package, including a new annealing-type algorithm and a new family of Gaussian Process-based SMBO methods, on four screening problems from HPOLib. We find that methods based on Gaussian Processes (GPs) are the most call-efficient. Vanilla GP-based methods using stationary RBF kernels and maximum likelihood kernel parameter estimation provide a near-perfect ability to optimize the benchmarks. Despite being slower than more heuristic baselines, a Theano-based GP-SMBO implementation requires at most a few seconds to produce a candidate evaluation point. We compare this vanilla approach to Hybrid Monte-Carlo integration of the kernel lengthscales and fail to find compelling advantages of this more expensive procedure.},
}
