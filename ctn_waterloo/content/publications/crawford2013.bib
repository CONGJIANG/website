@inproceedings {crawford2013,
  title={Biologically Plausible, Human-scale Knowledge Representation},
  booktitle={35th Annual Conference of the Cognitive Science Society},
  year={2013},
  pages={412--417},
  keywords={cleanup memory, knowledge representation, Semantic Pointer Architecture, vector symbolic architecture, WordNet},
  abstract={Several approaches to implementing symbol-like representations in neurally plausible models have been proposed. These approaches include binding through synchrony (Shastri & Ajjanagadde, 1993), mesh binding (van Der Velde & de Kamps, 2006), and conjunctive binding (Smolensky, 1990; Plate, 2003). Recent theoretical work has suggested that most of these methods will not scale well -- that is, they cannot encode structured representations that use any of the tens of thousands of terms in the adult lexicon without making implausible resource assumptions (Stewart & Eliasmith, 2011; Eliasmith, 2013). Here we present an approach that will scale appropriately, and which is based on neurally implementing a type of Vector Symbolic Architecture (VSA). Specifically, we construct a spiking neural network composed of about 2.5 million neurons that employs a VSA to encode and decode the main lexical relations in WordNet, a semantic network containing over 100,000 concepts (Fellbaum, 1998). We experimentally demonstrate the capabilities of our model by measuring its performance on three tasks which test its ability to accurately traverse the WordNet hierarchy, as well as to decode sentences employing any WordNet term while preserving the original lexical structure. We argue that these results show that our approach is uniquely well-suited to providing a biologically plausible, human-scale account of the structured representations that underwrite cognition.},
  author={Eric Crawford and Matthew Gingerich and Chris Eliasmith},
  pdf={http://mindmodeling.org/cogsci2013/papers/0099/paper0099.pdf},
  presentation={}
}
